<!DOCTYPE html>
<html>
<head>
<title>proj3.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="cs-739-madkv-project-3">CS 739 MadKV Project 3</h1>
<table>
<thead>
<tr>
<th><strong>Group Members</strong></th>
<th><strong>Name</strong></th>
<th><strong>Email</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Member 1</strong></td>
<td>Smit Shah</td>
<td>spshah25@wisc.edu</td>
</tr>
<tr>
<td><strong>Member 2</strong></td>
<td>Srihari Sridharan</td>
<td>srihari.sridharan@wisc.edu</td>
</tr>
</tbody>
</table>
<h2 id="design-walkthrough">Design Walkthrough</h2>
<p>We are going to implement Raft consensus protocol across all components. Due to resource constraints on CloudLab, our cluster setup is structured such that all components—including servers, its replicas, the cluster manager, and clients—are executed as separate processes within the same node. While this setup deviates from a distributed multi-node architecture, it allows us to simulate and test our system's behavior effectively without needing additional physical or virtual machines. This design choice ensures that we can evaluate the cluster’s coordination and request handling while adhering to the limitations of our testbed environment.</p>
<h3 id="design-walkthrough-code-structure--abstractions">Design Walkthrough:  Code Structure &amp; Abstractions</h3>
<p>Our system is composed of two major components: the Cluster Manager and Raft-based replicated servers. The Cluster Manager acts as a centralized coordination unit and does not maintain replicated state. It holds metadata and serves as a registration oracle, responsible for tracking the IP addresses of servers across partitions. Internally, it uses a map structure where the key is the partition ID, and the value is a vector&lt;pair&lt;string, bool&gt;&gt;—the pair indicating each server’s IP address and its registration status.</p>
<p>Upon receiving a configuration command, the Cluster Manager parses the IPs and stores them under the appropriate partition. A partition is considered fully registered once at least one server has successfully registered (boolean marked true) for that partition.</p>
<p>For the servers, we adopted a clean and modular Raft consensus protocol implementation. Each server can be in one of three states: FOLLOWER, LEADER, or CANDIDATE. We abstracted RPC communication into methods for AppendEntries and RequestVote, and we store metadata like currentTerm, votedFor, and the replicated log in memory. All logs are structured as entries containing the operation type (PUT, GET, SCAN , DELETE, SWAP), key-value data, term, and index.</p>
<p>Key background threads include:</p>
<pre><code>A heartbeat/election timeout monitor to trigger elections (T1)

A queue and execution manager for client requests (T2)

A retry loop for unacknowledged entries (T3)
</code></pre>
<p>This separation of responsibilities, combined with batching and concurrent RPCs to followers, ensured the design remained both extensible and fault-aware.
Replication Protocol Implementation – Easy, Hard, and Interesting</p>
<p>Easy:
Implementing the core Raft logic like leader election, log appending, and commit indexing followed well-documented procedures. The clear role-based design (Follower vs. Leader vs. Candidate) helped compartmentalize behavior cleanly.</p>
<p>Hard:
Handling concurrency—especially between election-time tasks (T1), live client request processing (T2), and retry mechanisms (T3)—was non-trivial. It was important to ensure consistency in log state while maintaining responsiveness. Another challenge was designing the retry logic for followers that lag or temporarily reject entries. Ensuring log consistency in these scenarios required careful coordination with Raft’s nextIndex and matchIndex updates.</p>
<p>Interesting:
We found the process of maintaining commitIndex stability and deriving it from the matchIndex of a majority particularly insightful. It was also interesting to observe how leader authority is established through AppendEntries (even empty NO-OP RPCs), and how followers validate historical consistency via prevLogIndex and prevLogTerm.
Node Failure Mitigation</p>
<p>The system's resilience stems from its strict adherence to Raft’s majority-based quorum logic. If a leader receives successful responses from a majority of followers for an AppendEntries RPC, it considers the entry committed—even if a minority of nodes are unresponsive. These lagging followers are handled gracefully:</p>
<pre><code>The leader retries failed AppendEntries with adjusted nextIndex.

If a follower repeatedly fails, it does not block consensus progress, maintaining availability.
</code></pre>
<p>If the majority fails, the leader steps down, and a new election is triggered. The election timeout thread ensures timely leadership reevaluation by tracking when the last heartbeat or vote was received.</p>
<p>On client side, we emphasized idempotent retry logic—clients are required to retry their requests until a definitive response is received. This accounts for transient failures, such as leader crashes or network partitions, ensuring client commands are eventually committed (or redirected to the correct leader).</p>
<p>The Cluster Manager, while currently a single point of failure, is designed with simplicity in mind and could be enhanced in future iterations using consensus-backed replication (e.g., multi-node Raft for manager state).</p>
<h2 id="api-design--model-summary">API Design / Model Summary</h2>
<p>Our Raft-based distributed system was built with a focus on clarity, correctness, and modular RPC-based coordination between servers. The following summarizes the key design aspects, data models, and RPC APIs implemented on the server side.
Server State and Data Model</p>
<p>Each server maintains a clearly defined state:</p>
<pre><code>FOLLOWER: Passive node, responds to RPCs.

CANDIDATE: Competes in elections after timeout.

LEADER: Handles client requests and coordinates replication.
</code></pre>
<p>We store Raft-related metadata in in-mmeory Raft class. The schema includes:</p>
<pre><code>currentTerm: Integer representing the latest known term.

votedFor: Replica ID that the server voted for in the current term.

log[]: An array of log entries. Each entry stores:

    term: Term in which the entry was created.

    index: Log index of the entry.

    command: One of PUT &lt;k&gt; &lt;v&gt;, SWAP &lt;k&gt; &lt;v&gt;, GET &lt;k&gt;, SCAN &lt;sk&gt; &lt;ek&gt; or DELETE &lt;k&gt;.
</code></pre>
<h3 id="appendentries-rpc">AppendEntries RPC</h3>
<p>Sender (Leader) Implementation:</p>
<pre><code>Invoked when a client sends a new command.

The leader appends the entry to its local log and concurrently sends AppendEntries RPCs to all followers.

Each follower receives the following fields:

    term: Leader’s current term.

    leaderId: ID of the sending leader.

    leaderIP: IP address for client redirection.

    prevLogIndex &amp; prevLogTerm: For log consistency checks.

    entries[]: Batched entries starting from prevLogIndex + 1.

    leaderCommit: Leader’s commit index.
</code></pre>
<p>Response Handling:</p>
<pre><code>If follower responds with success:

    Update matchIndex and nextIndex.

If follower responds with failure:

    Decrement nextIndex[follower] and retry.

Once all threads join the leader calculates commitIndex based on a majority of matchIndex values.

Entries between the old and new commitIndex are applied to both in-memory and state machines, and the response is returned to the client.
</code></pre>
<p>Receiver (Follower) Implementation:</p>
<pre><code>Rejects the request if term is outdated or the log is inconsistent at prevLogIndex.

Conflicting entries (same index, different term) are overwritten.

Appends any new entries.

Updates commitIndex if leader’s commit index is higher.

Applies all committed entries to its local state.
</code></pre>
<h3 id="requestvote-rpc">RequestVote RPC</h3>
<pre><code>Used during election when a server transitions to the CANDIDATE role.

Invoked when the electionTimeout elapses without receiving a heartbeat.

A background thread monitors time since last RPC or vote grant via a lastTimeWhenReceivedRPC timestamp.

The RPC includes:

    term: Candidate’s term.

    candidateId: ID of the requesting node.

    lastLogIndex and lastLogTerm: To ensure log freshness.
</code></pre>
<p>Response Logic:</p>
<pre><code>Servers grant vote if:

    The request’s term is newer.

    The candidate’s log is at least as up-to-date.

Each server only votes once per term.
</code></pre>
<p>Concurrency Consideration:</p>
<pre><code>AppendEntries and RequestVote are processed on the main thread.

To maintain atomicity, if a server is currently servicing a client request or log replication, runs in parallel. 
</code></pre>
<p>Client Interaction &amp; Redirection</p>
<pre><code>Only the leader accepts client commands.

If a follower receives a request, it responds with the leader’s IP.

Clients are required to implement retries in the event of leader crashes, as a command is only considered successful once committed and acknowledged by the leader.
</code></pre>
<p>This modular design ensures fault-tolerant replication, state consistency, and leader-driven coordination with a clear interface boundary for external clients and internal nodes.</p>
<h2 id="self-provided-testcases">Self-provided Testcases</h2>
<p>To validate the robustness and correctness of our RAFT-based replication system, we designed and executed the following failure-based test scenarios:</p>
<pre><code>Follower Failure Simulation
We simulated the failure of one or more follower replicas during normal operation. After issuing client commands to the leader, we intentionally terminated follower processes to ensure that the leader could still commit entries with a majority quorum. These tests verified that the system remains available and consistent despite partial replica failure and that retry logic on the leader correctly handled failed or lagging followers.

Leader Failure and Election Triggering
To test leader reelection, we gracefully and abruptly terminated the current leader while it was serving client requests. The remaining nodes detected the lack of heartbeat within the election timeout and independently initiated new elections. We verified that a new leader was successfully elected, client redirection occurred correctly, and log continuity was preserved across terms.

Failure Beyond Quorum Threshold
We simulated a more extreme scenario by killing replicas until the system no longer had a majority available. In this case, we verified that the leader could no longer make progress (i.e., client requests were rejected or blocked), and no new leader could be elected. This confirmed the system’s adherence to RAFT’s safety guarantees—no decisions were made without quorum.
</code></pre>
<p>These testcases collectively helped ensure that our implementation maintains consistency, fault-tolerance, and leadership stability under varying failure conditions.</p>
<h2 id="fuzz-testing">Fuzz Testing</h2>
<p><u>Parsed the following fuzz testing results:</u></p>
<table>
<thead>
<tr>
<th style="text-align:center">server_rf</th>
<th style="text-align:center">crashing</th>
<th style="text-align:center">outcome</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">no</td>
<td style="text-align:center">PASSED</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">yes</td>
<td style="text-align:center">PASSED</td>
</tr>
</tbody>
</table>
<p>You may be asked to run a crashing fuzz test during demo time.</p>
<h3 id="comments">Comments</h3>
<p>Our fuzz testing has consistently produced correct outcomes. However, during the development cycle, we did face issues with timeout of the fuzzer. Our code utilizes locks and all processes run on the same node. Due to this, our performance is on average subpar than the expectation. In this assignment since we are focusing on consistency and orrect data rather than performance, we decided to increase the timeout of the fuzzer by 10% to make sure our code succeeds.</p>
<p>Sometimes when we run concurrent processes on the same machine and when we kill the leader, our election triggers transparently. When the new leader takes charge and sends the AppendEntries command received from client, in our implementation we wait for all threads to join. This is a simplification of the original Raft protocol and sometimes leads to unexpected behaviours.</p>
<p>Since we do not have complete visibility into how the Rust-based fuzz tests are created and executed, adding debug statements and tracing the flow of execution has been difficult. Genreation of random testcases everytime makes it difficult to reproduce the same bug. The learning curve for debugging Rust proved to be steep, and given time constraints, we opted to focus on rerunning the service and client rather than diving deep into Rust debugging.</p>
<h2 id="ycsb-benchmarking">YCSB Benchmarking</h2>
<p><u>10 clients throughput/latency across workloads &amp; replication factors:</u></p>
<p><img src="plots-p3/ycsb-ten-clients.png" alt="ten-clients"></p>
<p><u>Agg. throughput trend vs. number of clients with different replication factors:</u></p>
<p><img src="plots-p3/ycsb-tput-trend.png" alt="tput-trend"></p>
<h3 id="comments">Comments</h3>
<p>Workload A-F with 10 Clients, Varying Replication Factor</p>
<p>We observe that rf=1 has lowest latency. This is because extra overhead for creating threads, waiting for threads, sending periodidc heartbeats, thread cleanup, is all nil. This latency is still higher than p2 assignment graph with 1 rf and 1 partition. This is because we do append entries in a log, sort, find maximu commit index.</p>
<p>Workload e has SCAN operations because it is ordered map and we iterate over all keys to push it in response. Absolute difference of the latencies with varying replication factors is constant. This indicates workload agnostic factor which in this project is the heartbeat sent periodically over the network.</p>
<p>The first benchmark (Image 1) was conducted across YCSB Workloads A to F using 10 concurrent clients while varying the replication factor (rf) between 1, 3, and 5.</p>
<pre><code>Throughput (Left chart):
We observed that the system achieves highest throughput with a single replica (1 rf) across all workloads, peaking near 2800 ops/sec. As the replication factor increases, throughput drops significantly due to the overhead of maintaining consistency across multiple replicas via Raft’s AppendEntries protocol. At 5 rf, the system achieves less than 500 ops/sec, highlighting the tradeoff between consistency and performance.

Average Latency (Middle chart):
Latency increases with replication. With 1 rf, average latency is as low as 4ms, while 5 rf experiences upward of 40ms, driven by the need to wait for majority acknowledgments before committing entries.

P99 Latency (Right chart):
The 99th percentile latency dramatically increases with replication. With 5 rf, p99 latency reaches up to 6 seconds in some workloads (notably B and E), indicating the long tail behavior induced by slow or unresponsive followers.
</code></pre>
<p>These results reflect how replication provides fault tolerance but adds latency and reduces throughput due to coordination costs.</p>
<p>Workload A, Scaling Clients (1 rf vs. 5 rf)
5rf throughput stays constant on increasing number of clients because server is the bottleneck. Creation of threads , management, cleanup heartbeats all add to overhead.
However,for 1rf client is the bottleneck. Our hunch is that it will have an inflection point after whichs erver becomes the bottleneck.</p>
<p>In the second benchmark (Image 2), we fixed the workload to YCSB-A and varied the number of clients from 1 to 30 for two configurations: 1 replica and 5 replicas.</p>
<pre><code>With 1 rf, the system scaled well. Throughput increased linearly with more clients, peaking above 3700 ops/sec with 30 clients, showing good concurrency support and minimal write coordination.

In contrast, with 5 rf, throughput remained flat and low (~200–300 ops/sec) regardless of client count. This confirms that replication is the bottleneck, and adding more clients does not improve performance due to serialization and quorum-based replication delays.
</code></pre>
<p>This experiment demonstrates that while our system handles increased load effectively in non-replicated scenarios, replication introduces a ceiling on scalability due to synchronous coordination overhead in Raft.</p>

</body>
</html>
